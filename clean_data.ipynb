{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Import libraries\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Connect to database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv('DB_NAME'),\n",
    "    user=os.getenv('DB_USER'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=os.getenv('DB_PORT')\n",
    ")\n",
    "print(\"Connected to database successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8998/276969155.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  total_rows = pd.read_sql(count_query, conn).iloc[0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to process: 4,902,536\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Get total count first\n",
    "count_query = \"\"\"\n",
    "SELECT  COUNT(*) \n",
    "FROM \"hacker_news\".\"items\" \n",
    "WHERE score IS NOT NULL \n",
    "  AND title IS NOT NULL \n",
    "  AND by IS NOT NULL\n",
    "  AND type = 'story'\n",
    "\"\"\"\n",
    "\n",
    "total_rows = pd.read_sql(count_query, conn).iloc[0, 0]\n",
    "print(f\"Total rows to process: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process in 99 chunks of 50,000 rows each\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Set chunk parameters\n",
    "chunk_size = 50000  # Adjust this if needed\n",
    "num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size > 0 else 0)\n",
    "print(f\"Will process in {num_chunks} chunks of {chunk_size:,} rows each\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chunked loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8998/2324593453.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  chunk_df = pd.read_sql(chunk_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk loaded: 50,000 rows\n",
      "Memory usage: 15.0 MB\n",
      "First chunk saved as workspace/data/chunks/chunk_0.parquet\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "print(\"Starting chunked loading...\")\n",
    "\n",
    "# CELL 6: Load first chunk (test)\n",
    "chunk_query = f\"\"\"\n",
    "SELECT id, by, time, url, score, title, descendants\n",
    "FROM \"hacker_news\".\"items\" \n",
    "WHERE score IS NOT NULL \n",
    "  AND title IS NOT NULL \n",
    "  AND by IS NOT NULL\n",
    "  AND type = 'story'\n",
    "ORDER BY time\n",
    "LIMIT {chunk_size} OFFSET 0\n",
    "\"\"\"\n",
    "\n",
    "chunk_df = pd.read_sql(chunk_query, conn)\n",
    "print(f\"First chunk loaded: {len(chunk_df):,} rows\")\n",
    "print(f\"Memory usage: {chunk_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Save first chunk\n",
    "chunk_filename = 'workspace/data/chunks/chunk_0.parquet'\n",
    "chunk_df.to_parquet(chunk_filename, index=False)\n",
    "all_chunks.append(chunk_filename)  # Store filename instead of dataframe\n",
    "print(f\"First chunk saved as {chunk_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8998/3387744160.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  chunk_df = pd.read_sql(chunk_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2/99 saved: 50,000 rows -> workspace/data/chunks/chunk_1.parquet\n",
      "Chunk 3/99 saved: 50,000 rows -> workspace/data/chunks/chunk_2.parquet\n",
      "Chunk 4/99 saved: 50,000 rows -> workspace/data/chunks/chunk_3.parquet\n",
      "Chunk 5/99 saved: 50,000 rows -> workspace/data/chunks/chunk_4.parquet\n",
      "Chunk 6/99 saved: 50,000 rows -> workspace/data/chunks/chunk_5.parquet\n",
      "Chunk 7/99 saved: 50,000 rows -> workspace/data/chunks/chunk_6.parquet\n",
      "Chunk 8/99 saved: 50,000 rows -> workspace/data/chunks/chunk_7.parquet\n",
      "Chunk 9/99 saved: 50,000 rows -> workspace/data/chunks/chunk_8.parquet\n",
      "Chunk 10/99 saved: 50,000 rows -> workspace/data/chunks/chunk_9.parquet\n",
      "Chunk 11/99 saved: 50,000 rows -> workspace/data/chunks/chunk_10.parquet\n",
      "Chunk 12/99 saved: 50,000 rows -> workspace/data/chunks/chunk_11.parquet\n",
      "Chunk 13/99 saved: 50,000 rows -> workspace/data/chunks/chunk_12.parquet\n",
      "Chunk 14/99 saved: 50,000 rows -> workspace/data/chunks/chunk_13.parquet\n",
      "Chunk 15/99 saved: 50,000 rows -> workspace/data/chunks/chunk_14.parquet\n",
      "Chunk 16/99 saved: 50,000 rows -> workspace/data/chunks/chunk_15.parquet\n",
      "Chunk 17/99 saved: 50,000 rows -> workspace/data/chunks/chunk_16.parquet\n",
      "Chunk 18/99 saved: 50,000 rows -> workspace/data/chunks/chunk_17.parquet\n",
      "Chunk 19/99 saved: 50,000 rows -> workspace/data/chunks/chunk_18.parquet\n",
      "Chunk 20/99 saved: 50,000 rows -> workspace/data/chunks/chunk_19.parquet\n",
      "Chunk 21/99 saved: 50,000 rows -> workspace/data/chunks/chunk_20.parquet\n",
      "Chunk 22/99 saved: 50,000 rows -> workspace/data/chunks/chunk_21.parquet\n",
      "Chunk 23/99 saved: 50,000 rows -> workspace/data/chunks/chunk_22.parquet\n",
      "Chunk 24/99 saved: 50,000 rows -> workspace/data/chunks/chunk_23.parquet\n",
      "Chunk 25/99 saved: 50,000 rows -> workspace/data/chunks/chunk_24.parquet\n",
      "Chunk 26/99 saved: 50,000 rows -> workspace/data/chunks/chunk_25.parquet\n",
      "Chunk 27/99 saved: 50,000 rows -> workspace/data/chunks/chunk_26.parquet\n",
      "Chunk 28/99 saved: 50,000 rows -> workspace/data/chunks/chunk_27.parquet\n",
      "Chunk 29/99 saved: 50,000 rows -> workspace/data/chunks/chunk_28.parquet\n",
      "Chunk 30/99 saved: 50,000 rows -> workspace/data/chunks/chunk_29.parquet\n",
      "Chunk 31/99 saved: 50,000 rows -> workspace/data/chunks/chunk_30.parquet\n",
      "Chunk 32/99 saved: 50,000 rows -> workspace/data/chunks/chunk_31.parquet\n",
      "Chunk 33/99 saved: 50,000 rows -> workspace/data/chunks/chunk_32.parquet\n",
      "Chunk 34/99 saved: 50,000 rows -> workspace/data/chunks/chunk_33.parquet\n",
      "Chunk 35/99 saved: 50,000 rows -> workspace/data/chunks/chunk_34.parquet\n",
      "Chunk 36/99 saved: 50,000 rows -> workspace/data/chunks/chunk_35.parquet\n",
      "Chunk 37/99 saved: 50,000 rows -> workspace/data/chunks/chunk_36.parquet\n",
      "Chunk 38/99 saved: 50,000 rows -> workspace/data/chunks/chunk_37.parquet\n",
      "Chunk 39/99 saved: 50,000 rows -> workspace/data/chunks/chunk_38.parquet\n",
      "Chunk 40/99 saved: 50,000 rows -> workspace/data/chunks/chunk_39.parquet\n",
      "Chunk 41/99 saved: 50,000 rows -> workspace/data/chunks/chunk_40.parquet\n",
      "Chunk 42/99 saved: 50,000 rows -> workspace/data/chunks/chunk_41.parquet\n",
      "Chunk 43/99 saved: 50,000 rows -> workspace/data/chunks/chunk_42.parquet\n",
      "Chunk 44/99 saved: 50,000 rows -> workspace/data/chunks/chunk_43.parquet\n",
      "Chunk 45/99 saved: 50,000 rows -> workspace/data/chunks/chunk_44.parquet\n",
      "Chunk 46/99 saved: 50,000 rows -> workspace/data/chunks/chunk_45.parquet\n",
      "Chunk 47/99 saved: 50,000 rows -> workspace/data/chunks/chunk_46.parquet\n",
      "Chunk 48/99 saved: 50,000 rows -> workspace/data/chunks/chunk_47.parquet\n",
      "Chunk 49/99 saved: 50,000 rows -> workspace/data/chunks/chunk_48.parquet\n",
      "Chunk 50/99 saved: 50,000 rows -> workspace/data/chunks/chunk_49.parquet\n",
      "Chunk 51/99 saved: 50,000 rows -> workspace/data/chunks/chunk_50.parquet\n",
      "Chunk 52/99 saved: 50,000 rows -> workspace/data/chunks/chunk_51.parquet\n",
      "Chunk 53/99 saved: 50,000 rows -> workspace/data/chunks/chunk_52.parquet\n",
      "Chunk 54/99 saved: 50,000 rows -> workspace/data/chunks/chunk_53.parquet\n",
      "Chunk 55/99 saved: 50,000 rows -> workspace/data/chunks/chunk_54.parquet\n",
      "Chunk 56/99 saved: 50,000 rows -> workspace/data/chunks/chunk_55.parquet\n",
      "Chunk 57/99 saved: 50,000 rows -> workspace/data/chunks/chunk_56.parquet\n",
      "Chunk 58/99 saved: 50,000 rows -> workspace/data/chunks/chunk_57.parquet\n",
      "Chunk 59/99 saved: 50,000 rows -> workspace/data/chunks/chunk_58.parquet\n",
      "Chunk 60/99 saved: 50,000 rows -> workspace/data/chunks/chunk_59.parquet\n",
      "Chunk 61/99 saved: 50,000 rows -> workspace/data/chunks/chunk_60.parquet\n",
      "Chunk 62/99 saved: 50,000 rows -> workspace/data/chunks/chunk_61.parquet\n",
      "Chunk 63/99 saved: 50,000 rows -> workspace/data/chunks/chunk_62.parquet\n",
      "Chunk 64/99 saved: 50,000 rows -> workspace/data/chunks/chunk_63.parquet\n",
      "Chunk 65/99 saved: 50,000 rows -> workspace/data/chunks/chunk_64.parquet\n",
      "Chunk 66/99 saved: 50,000 rows -> workspace/data/chunks/chunk_65.parquet\n",
      "Chunk 67/99 saved: 50,000 rows -> workspace/data/chunks/chunk_66.parquet\n",
      "Chunk 68/99 saved: 50,000 rows -> workspace/data/chunks/chunk_67.parquet\n",
      "Chunk 69/99 saved: 50,000 rows -> workspace/data/chunks/chunk_68.parquet\n",
      "Chunk 70/99 saved: 50,000 rows -> workspace/data/chunks/chunk_69.parquet\n",
      "Chunk 71/99 saved: 50,000 rows -> workspace/data/chunks/chunk_70.parquet\n",
      "Chunk 72/99 saved: 50,000 rows -> workspace/data/chunks/chunk_71.parquet\n",
      "Chunk 73/99 saved: 50,000 rows -> workspace/data/chunks/chunk_72.parquet\n",
      "Chunk 74/99 saved: 50,000 rows -> workspace/data/chunks/chunk_73.parquet\n",
      "Chunk 75/99 saved: 50,000 rows -> workspace/data/chunks/chunk_74.parquet\n",
      "Chunk 76/99 saved: 50,000 rows -> workspace/data/chunks/chunk_75.parquet\n",
      "Chunk 77/99 saved: 50,000 rows -> workspace/data/chunks/chunk_76.parquet\n",
      "Chunk 78/99 saved: 50,000 rows -> workspace/data/chunks/chunk_77.parquet\n",
      "Chunk 79/99 saved: 50,000 rows -> workspace/data/chunks/chunk_78.parquet\n",
      "Chunk 80/99 saved: 50,000 rows -> workspace/data/chunks/chunk_79.parquet\n",
      "Chunk 81/99 saved: 50,000 rows -> workspace/data/chunks/chunk_80.parquet\n",
      "Chunk 82/99 saved: 50,000 rows -> workspace/data/chunks/chunk_81.parquet\n",
      "Chunk 83/99 saved: 50,000 rows -> workspace/data/chunks/chunk_82.parquet\n",
      "Chunk 84/99 saved: 50,000 rows -> workspace/data/chunks/chunk_83.parquet\n",
      "Chunk 85/99 saved: 50,000 rows -> workspace/data/chunks/chunk_84.parquet\n",
      "Chunk 86/99 saved: 50,000 rows -> workspace/data/chunks/chunk_85.parquet\n",
      "Chunk 87/99 saved: 50,000 rows -> workspace/data/chunks/chunk_86.parquet\n",
      "Chunk 88/99 saved: 50,000 rows -> workspace/data/chunks/chunk_87.parquet\n",
      "Chunk 89/99 saved: 50,000 rows -> workspace/data/chunks/chunk_88.parquet\n",
      "Chunk 90/99 saved: 50,000 rows -> workspace/data/chunks/chunk_89.parquet\n",
      "Chunk 91/99 saved: 50,000 rows -> workspace/data/chunks/chunk_90.parquet\n",
      "Chunk 92/99 saved: 50,000 rows -> workspace/data/chunks/chunk_91.parquet\n",
      "Chunk 93/99 saved: 50,000 rows -> workspace/data/chunks/chunk_92.parquet\n",
      "Chunk 94/99 saved: 50,000 rows -> workspace/data/chunks/chunk_93.parquet\n",
      "Chunk 95/99 saved: 50,000 rows -> workspace/data/chunks/chunk_94.parquet\n",
      "Chunk 96/99 saved: 50,000 rows -> workspace/data/chunks/chunk_95.parquet\n",
      "Chunk 97/99 saved: 50,000 rows -> workspace/data/chunks/chunk_96.parquet\n",
      "Chunk 98/99 saved: 50,000 rows -> workspace/data/chunks/chunk_97.parquet\n",
      "Chunk 99/99 saved: 2,536 rows -> workspace/data/chunks/chunk_98.parquet\n",
      "Completed loading 99 chunks\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Load remaining chunks in loop\n",
    "for i in range(1, num_chunks):\n",
    "    offset = i * chunk_size\n",
    "    \n",
    "    chunk_query = f\"\"\"\n",
    "    SELECT id, by, time, url, score, title, descendants\n",
    "    FROM \"hacker_news\".\"items\" \n",
    "    WHERE score IS NOT NULL \n",
    "      AND title IS NOT NULL \n",
    "      AND by IS NOT NULL\n",
    "      AND type = 'story'\n",
    "    ORDER BY time\n",
    "    LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        chunk_df = pd.read_sql(chunk_query, conn)\n",
    "        if len(chunk_df) > 0:\n",
    "            # Save chunk immediately\n",
    "            chunk_filename = f'workspace/data/chunks/chunk_{i}.parquet'\n",
    "            chunk_df.to_parquet(chunk_filename, index=False)\n",
    "            all_chunks.append(chunk_filename)\n",
    "            print(f\"Chunk {i+1}/{num_chunks} saved: {len(chunk_df):,} rows -> {chunk_filename}\")\n",
    "        else:\n",
    "            print(f\"Chunk {i+1} was empty, stopping\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunk {i+1}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Completed loading {len(all_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all chunks...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombining all chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m combined_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_chunks\u001b[49m:\n\u001b[1;32m      6\u001b[0m     chunk_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(chunk_file)\n\u001b[1;32m      7\u001b[0m     combined_chunks\u001b[38;5;241m.\u001b[39mappend(chunk_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# CELL 8: Combine all chunks into final dataset\n",
    "print(\"Combining all chunks...\")\n",
    "combined_chunks = []\n",
    "\n",
    "for chunk_file in all_chunks:\n",
    "    chunk_df = pd.read_parquet(chunk_file)\n",
    "    combined_chunks.append(chunk_df)\n",
    "    print(f\"Loaded {chunk_file}: {len(chunk_df):,} rows\")\n",
    "\n",
    "final_df = pd.concat(combined_chunks, ignore_index=True)\n",
    "print(f\"Final combined dataset: {len(final_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Save final dataset and cleanup\n",
    "final_df.to_parquet('workspace/data/hackernews_full_data.parquet', index=False)\n",
    "print(\"Final dataset saved as hackernews_full_data.parquet\")\n",
    "\n",
    "# Clean up individual chunk files\n",
    "for chunk_file in all_chunks:\n",
    "    try:\n",
    "        os.remove(chunk_file)\n",
    "        print(f\"Removed {chunk_file}\")\n",
    "    except:\n",
    "        print(f\"Could not remove {chunk_file}\")\n",
    "\n",
    "print(\"Cleanup completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Verify final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {final_df.shape}\")\n",
    "print(f\"Memory usage: {final_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Date range: {final_df['time'].min()} to {final_df['time'].max()}\")\n",
    "print(f\"Score range: {final_df['score'].min()} to {final_df['score'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Close database connection\n",
    "conn.close()\n",
    "print(\"Database connection closed\")\n",
    "print(\"Data loading complete! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Quick data preview\n",
    "print(\"Sample data:\")\n",
    "final_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
