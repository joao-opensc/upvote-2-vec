{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Word2Vec with Hyperparameter Optimization\n",
    "Clean, minimal implementation focused on finding the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wandb setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup wandb directories\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = '/tmp/wandb'\n",
    "os.environ['WANDB_CACHE_DIR'] = '/tmp/wandb_cache'\n",
    "os.environ['WANDB_DATA_DIR'] = '/tmp/wandb_data'\n",
    "os.makedirs('/tmp/wandb', exist_ok=True)\n",
    "os.makedirs('/tmp/wandb_cache', exist_ok=True) \n",
    "os.makedirs('/tmp/wandb_data', exist_ok=True)\n",
    "\n",
    "print(\"✅ Wandb setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 17,005,207\n",
      "Vocabulary size: 71,290\n",
      "Training words: 16,718,844\n",
      "Sample words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# Load and process text8 data\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# Read text8 dataset\n",
    "with open('workspace/data/text8', 'r') as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "\n",
    "# Build vocabulary (words appearing at least 5 times)\n",
    "word_counts = collections.Counter(words)\n",
    "vocabulary = {word: count for word, count in word_counts.items() if count >= 5}\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary.keys())}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary):,}\")\n",
    "\n",
    "# Convert to indices (subsample for training speed)\n",
    "indexed_words = [word_to_index[word] for word in words if word in vocabulary]\n",
    "\n",
    "print(f\"Training words: {len(indexed_words):,}\")\n",
    "print(f\"Sample words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and data functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Simple Word2Vec Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        # Good initialization\n",
    "        nn.init.xavier_uniform_(self.embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        # Bag of words: average context embeddings\n",
    "        embedded = self.embeddings(context)\n",
    "        hidden = torch.tanh(torch.mean(embedded, dim=1))\n",
    "        output = self.linear(hidden)\n",
    "        return output\n",
    "\n",
    "def create_training_data(indexed_words, window_size, max_examples=None):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "\n",
    "    if max_examples is None:\n",
    "        end_idx = len(indexed_words) - window_size\n",
    "    else:\n",
    "        end_idx = min(len(indexed_words) - window_size, max_examples + window_size)\n",
    "\n",
    "    for i in range(window_size, end_idx):\n",
    "        context = (indexed_words[i - window_size:i] +\n",
    "                   indexed_words[i + 1:i + window_size + 1])\n",
    "        target = indexed_words[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "    return torch.tensor(contexts), torch.tensor(targets)\n",
    "\n",
    "\n",
    "print(\"✅ Model and data functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Sweep configuration ready!\n",
      "Will test: ['learning_rate', 'batch_size', 'embedding_dim', 'window_size', 'optimizer', 'weight_decay']\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Sweep\n",
    "import wandb\n",
    "\n",
    "# Adjusted sweep configuration for large dataset (16.7M words)\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Smart parameter search\n",
    "    'metric': {'name': 'best_loss', 'goal': 'minimize'},\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.0005,  # Lower minimum for fine-tuning large dataset\n",
    "            'max': 0.2      # Higher maximum since large dataset can handle it\n",
    "        },\n",
    "        'batch_size': {'values': [128, 256, 512, 1024, 2048]},  # Larger batches for 16.7M words\n",
    "        'embedding_dim': {'values': [50, 100, 200, 300, 512]}, # Larger embeddings supported by more data\n",
    "        'window_size': {'values': [4, 5, 6, 8]},           # Include larger context window\n",
    "        'optimizer': {'values': ['adam', 'sgd', 'adamw']}, # Add AdamW for better regularization\n",
    "        'weight_decay': {\n",
    "            'distribution': 'log_uniform_values', \n",
    "            'min': 1e-6,\n",
    "            'max': 1e-2  # Higher weight decay for larger model/dataset\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Training function for wandb sweep\"\"\"\n",
    "    # Initialize wandb\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    \n",
    "    print(f\"Testing: lr={config.learning_rate:.4f}, batch={config.batch_size}, \"\n",
    "          f\"embed={config.embedding_dim}, window={config.window_size}\")\n",
    "    \n",
    "    # Create training data - use more examples for meaningful results with large dataset\n",
    "    contexts, targets = create_training_data(\n",
    "        indexed_words, \n",
    "        config.window_size,\n",
    "        max_examples=10000  # Increased for large dataset (16.7M words)\n",
    "    )\n",
    "\n",
    "    print(f\"💪 Training with {len(contexts):,} examples (not all available data!)\")\n",
    "    \n",
    "    # Setup training\n",
    "    dataset = TensorDataset(contexts, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleWord2Vec(len(vocabulary), config.embedding_dim).to(device)\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), \n",
    "                              lr=config.learning_rate, \n",
    "                              weight_decay=config.weight_decay)\n",
    "    elif config.optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                               lr=config.learning_rate, \n",
    "                               weight_decay=config.weight_decay)\n",
    "    else:  # sgd\n",
    "        optimizer = optim.SGD(model.parameters(), \n",
    "                             lr=config.learning_rate,\n",
    "                             momentum=0.9, \n",
    "                             weight_decay=config.weight_decay)\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop (adjusted for larger dataset)\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(15):  # More epochs for larger dataset\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_contexts, batch_targets in dataloader:\n",
    "            batch_contexts = batch_contexts.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_contexts)\n",
    "            loss = loss_function(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        best_loss = min(best_loss, avg_loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'avg_loss': avg_loss,\n",
    "            'best_loss': best_loss\n",
    "        })\n",
    "        \n",
    "        # Early stopping if converging well\n",
    "        if avg_loss < 3.0:\n",
    "            break\n",
    "    \n",
    "    # Final result\n",
    "    wandb.log({'final_best_loss': best_loss})\n",
    "    run.finish()\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "print(\"🎯 Sweep configuration ready!\")\n",
    "print(f\"Will test: {list(sweep_config['parameters'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter sweep\n",
    "print(\"🚀 Starting hyperparameter sweep...\")\n",
    "\n",
    "# Create and run sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"simple-word2vec\")\n",
    "print(f\"📊 Sweep ID: {sweep_id}\")\n",
    "print(f\"🌐 Monitor at: https://wandb.ai/{wandb.api.default_entity}/simple-word2vec/sweeps/{sweep_id}\")\n",
    "\n",
    "# Run 8 experiments\n",
    "wandb.agent(sweep_id, train_model, count=10)\n",
    "\n",
    "print(\"✅ Sweep complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sweep_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Get best parameters from your sweep\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m best_config \u001b[38;5;241m=\u001b[39m get_best_parameters(\u001b[43msweep_id\u001b[49m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_config:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m💡 Copy these parameters for your final training!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sweep_id' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "def get_best_parameters(sweep_id):\n",
    "    \"\"\"Extract best parameters from completed sweep\"\"\"\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"{wandb.api.default_entity}/simple-word2vec/{sweep_id}\")\n",
    "    \n",
    "    # Find best run\n",
    "    best_run = None\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    print(f\"📊 Analyzing {len(sweep.runs)} runs...\")\n",
    "    \n",
    "    for run in sweep.runs:\n",
    "        if run.state == \"finished\":\n",
    "            loss = run.summary.get('best_loss', float('inf'))\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_run = run\n",
    "    \n",
    "    if best_run:\n",
    "        print(f\"\\n🏆 BEST PARAMETERS (Loss: {best_loss:.4f})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        best_config = {}\n",
    "        for key, value in best_run.config.items():\n",
    "            best_config[key] = value\n",
    "            print(f\"{key:15}: {value}\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        return best_config\n",
    "    else:\n",
    "        print(\"❌ No successful runs found!\")\n",
    "        return None\n",
    "\n",
    "# Get best parameters from your sweep\n",
    "best_config = get_best_parameters(sweep_id)\n",
    "\n",
    "if best_config:\n",
    "    print(\"\\n💡 Copy these parameters for your final training!\")\n",
    "    print(f\"Best config: {best_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Manual configuration set!\n",
      "==================================================\n",
      "optimizer      : adam\n",
      "batch_size     : 512\n",
      "window_size    : 5\n",
      "weight_decay   : 1e-05\n",
      "embedding_dim  : 200\n",
      "learning_rate  : 0.02\n",
      "==================================================\n",
      "\n",
      "💡 You can modify these values above and re-run this cell\n",
      "   to experiment with different parameter combinations.\n"
     ]
    }
   ],
   "source": [
    "# Manual Configuration - Best Parameters\n",
    "# Based on sweep results, you can modify these values as needed\n",
    "\n",
    "best_config = {\n",
    "    'optimizer': 'adam',           # Best performing optimizer\n",
    "    'batch_size': 512,             # Large batch size for stable training\n",
    "    'window_size': 5,               # Context window size\n",
    "    'weight_decay': 1e-5,         # Regularization strength\n",
    "    'embedding_dim': 200,           # Embedding dimensions\n",
    "    'learning_rate': 0.02       # Learning rate\n",
    "}\n",
    "\n",
    "print(\"🎯 Manual configuration set!\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in best_config.items():\n",
    "    print(f\"{key:15}: {value}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n💡 You can modify these values above and re-run this cell\")\n",
    "print(\"   to experiment with different parameter combinations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training final model with best parameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/wandb/run-20250611_103800-6u2ljj5m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/6u2ljj5m' target=\"_blank\">final-training</a></strong> to <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/simple-word2vec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/6u2ljj5m' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/6u2ljj5m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1,000,000 examples...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m outputs \u001b[38;5;241m=\u001b[39m final_model(batch_contexts)\n\u001b[1;32m     63\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, batch_targets)\n\u001b[0;32m---> 64\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(final_model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     67\u001b[0m final_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Final training with best parameters and wandb logging\n",
    "if best_config:\n",
    "    print(\"🚀 Training final model with best parameters...\")\n",
    "    \n",
    "    # Initialize wandb run for final training\n",
    "    final_run = wandb.init(\n",
    "        project=\"simple-word2vec\",\n",
    "        name=\"final-training\",\n",
    "        config=best_config,\n",
    "        tags=[\"final\", \"best-params\"]\n",
    "    )\n",
    "    \n",
    "    # Use manageable amount of data for final model (1M examples - good balance)\n",
    "    final_contexts, final_targets = create_training_data(\n",
    "        indexed_words, \n",
    "        best_config['window_size'],\n",
    "        max_examples=1000000  # 1M examples - much more than sweep but manageable\n",
    "    )\n",
    "    \n",
    "    # Setup final training\n",
    "    final_dataset = TensorDataset(final_contexts, final_targets)\n",
    "    final_dataloader = DataLoader(final_dataset, \n",
    "                                 batch_size=best_config['batch_size'], \n",
    "                                 shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    final_model = SimpleWord2Vec(len(vocabulary), best_config['embedding_dim']).to(device)\n",
    "    \n",
    "    # Use best optimizer settings\n",
    "    if best_config['optimizer'] == 'adam':\n",
    "        final_optimizer = optim.Adam(final_model.parameters(), \n",
    "                                   lr=best_config['learning_rate'], \n",
    "                                   weight_decay=best_config['weight_decay'])\n",
    "    elif best_config['optimizer'] == 'adamw':\n",
    "        final_optimizer = optim.AdamW(final_model.parameters(), \n",
    "                                    lr=best_config['learning_rate'], \n",
    "                                    weight_decay=best_config['weight_decay'])\n",
    "    else:  # sgd\n",
    "        final_optimizer = optim.SGD(final_model.parameters(), \n",
    "                                  lr=best_config['learning_rate'],\n",
    "                                  momentum=0.9, \n",
    "                                  weight_decay=best_config['weight_decay'])\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train final model (more epochs)\n",
    "    final_model.train()\n",
    "    print(f\"Training with {len(final_contexts):,} examples...\")\n",
    "    \n",
    "    # Track losses for plotting\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(20):  # More epochs for final model\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch_contexts, batch_targets in final_dataloader:\n",
    "            batch_contexts = batch_contexts.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            final_optimizer.zero_grad()\n",
    "            outputs = final_model(batch_contexts)\n",
    "            loss = loss_function(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
    "            final_optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Log batch loss every 50 batches for detailed tracking\n",
    "            if batch_count % 50 == 0:\n",
    "                wandb.log({\n",
    "                    'batch_loss': loss.item(),\n",
    "                    'batch': epoch * len(final_dataloader) + batch_count\n",
    "                })\n",
    "        \n",
    "        avg_loss = epoch_loss / len(final_dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Log epoch metrics to wandb\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': avg_loss,\n",
    "            'learning_rate': final_optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: Loss = {avg_loss:.4f}\")\n",
    "        \n",
    "        if avg_loss < 2.0:\n",
    "            print(f\"🎯 Converged at epoch {epoch}!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n🎉 Final model training complete!\")\n",
    "    print(f\"📈 Final loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Log final metrics\n",
    "    wandb.log({\n",
    "        'final_train_loss': avg_loss,\n",
    "        'total_epochs': epoch + 1,\n",
    "        'convergence_epoch': epoch if avg_loss < 2.0 else None\n",
    "    })\n",
    "    \n",
    "    # Save embeddings\n",
    "    embeddings = final_model.embeddings.weight.detach().cpu().numpy()\n",
    "    print(f\"📊 Embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Quick similarity test\n",
    "    def find_similar_words(word, top_k=5):\n",
    "        if word not in word_to_index:\n",
    "            return f\"'{word}' not in vocabulary\"\n",
    "        \n",
    "        word_idx = word_to_index[word]\n",
    "        word_embed = embeddings[word_idx]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarities = np.dot(embeddings, word_embed) / (\n",
    "            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(word_embed)\n",
    "        )\n",
    "        \n",
    "        # Get top similar words\n",
    "        top_indices = np.argsort(similarities)[::-1][1:top_k+1]  # Skip the word itself\n",
    "        \n",
    "        similar_words = [(index_to_word[idx], similarities[idx]) \n",
    "                        for idx in top_indices]\n",
    "        return similar_words\n",
    "    \n",
    "    # Test similarity and log examples\n",
    "    test_words = ['king', 'computer', 'big', 'good']\n",
    "    print(\"\\n🔍 Word similarity test:\")\n",
    "    similarity_results = {}\n",
    "    \n",
    "    for word in test_words:\n",
    "        similar = find_similar_words(word, 3)\n",
    "        if isinstance(similar, str):\n",
    "            print(f\"{word}: {similar}\")\n",
    "            similarity_results[word] = similar\n",
    "        else:\n",
    "            similar_str = ', '.join([f\"{w}({s:.3f})\" for w, s in similar])\n",
    "            print(f\"{word}: {similar_str}\")\n",
    "            similarity_results[word] = similar_str\n",
    "    \n",
    "    # Log similarity results to wandb\n",
    "    wandb.log({\"word_similarities\": similarity_results})\n",
    "    \n",
    "    # Create a simple loss plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('Final Model Training Loss', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log the plot to wandb\n",
    "    wandb.log({\"training_loss_plot\": wandb.Image(plt)})\n",
    "    plt.show()\n",
    "    \n",
    "    # Finish the wandb run\n",
    "    final_run.finish()\n",
    "    \n",
    "    print(f\"🌐 View final training at: {final_run.url}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No best config found. Run the sweep first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving trained Word2Vec model and embeddings...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 1. Save the embeddings as numpy array\u001b[39;00m\n\u001b[1;32m     16\u001b[0m embeddings_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec_embeddings.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m np\u001b[38;5;241m.\u001b[39msave(embeddings_path, \u001b[43membeddings\u001b[49m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# 💾 SAVE TRAINED MODEL AND EMBEDDINGS FOR FUTURE USE\n",
    "import numpy as np\n",
    "print(\"💾 Saving trained Word2Vec model and embeddings...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create a directory for saved models\n",
    "save_dir = \"workspace/word2vec_trained_model/model_1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save the embeddings as numpy array\n",
    "embeddings_path = os.path.join(save_dir, \"word2vec_embeddings.npy\")\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"✅ Saved embeddings: {embeddings_path}\")\n",
    "print(f\"   Shape: {embeddings.shape}\")\n",
    "\n",
    "# 2. Save vocabulary mappings\n",
    "vocab_path = os.path.join(save_dir, \"word2vec_vocab.pkl\")\n",
    "vocab_data = {\n",
    "    'word_to_index': word_to_index,\n",
    "    'index_to_word': index_to_word,\n",
    "    'vocabulary': vocabulary,\n",
    "    'vocab_size': len(vocabulary)\n",
    "}\n",
    "\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab_data, f)\n",
    "print(f\"✅ Saved vocabulary: {vocab_path}\")\n",
    "print(f\"   Vocabulary size: {len(vocabulary):,} words\")\n",
    "\n",
    "# 3. Save the full model state\n",
    "model_path = os.path.join(save_dir, \"word2vec_model.pth\")\n",
    "model_save_data = {\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'model_config': {\n",
    "        'vocab_size': len(vocabulary),\n",
    "        'embedding_dim': best_config['embedding_dim'],\n",
    "        'architecture': 'SimpleWord2Vec_CBOW'\n",
    "    },\n",
    "    'training_config': best_config,\n",
    "    'training_info': {\n",
    "        'final_loss': avg_loss,\n",
    "        'total_epochs': epoch + 1,\n",
    "        'training_examples': len(final_contexts),\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(model_save_data, model_path)\n",
    "print(f\"✅ Saved model state: {model_path}\")\n",
    "\n",
    "# 4. Save human-readable configuration\n",
    "config_path = os.path.join(save_dir, \"model_info.json\")\n",
    "model_info = {\n",
    "    \"model_name\": \"SimpleWord2Vec_CBOW\",\n",
    "    \"embedding_dimensions\": best_config['embedding_dim'],\n",
    "    \"vocabulary_size\": len(vocabulary),\n",
    "    \"training_examples\": len(final_contexts),\n",
    "    \"final_loss\": float(avg_loss),\n",
    "    \"training_epochs\": epoch + 1,\n",
    "    \"best_hyperparameters\": {\n",
    "        \"learning_rate\": best_config['learning_rate'],\n",
    "        \"batch_size\": best_config['batch_size'],\n",
    "        \"window_size\": best_config['window_size'],\n",
    "        \"optimizer\": best_config['optimizer'],\n",
    "        \"weight_decay\": best_config['weight_decay']\n",
    "    },\n",
    "    \"dataset\": \"text8\",\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"usage_instructions\": {\n",
    "        \"load_embeddings\": \"embeddings = np.load('word2vec_embeddings.npy')\",\n",
    "        \"load_vocab\": \"with open('word2vec_vocab.pkl', 'rb') as f: vocab = pickle.load(f)\",\n",
    "        \"load_model\": \"checkpoint = torch.load('word2vec_model.pth')\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "print(f\"✅ Saved model info: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 LOAD TRAINED MODEL AND CONTINUE TRAINING\n",
    "print(\"🔄 Loading trained Word2Vec model for continued training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# Path to saved model\n",
    "model_dir = \"workspace/word2vec_trained_model/model_1\"\n",
    "\n",
    "# 1. Load vocabulary\n",
    "print(\"📖 Loading vocabulary...\")\n",
    "vocab_path = os.path.join(model_dir, \"word2vec_vocab.pkl\")\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "word_to_index = vocab_data['word_to_index']\n",
    "index_to_word = vocab_data['index_to_word']\n",
    "vocabulary = vocab_data['vocabulary']\n",
    "\n",
    "print(f\"✅ Loaded vocabulary: {len(vocabulary):,} words\")\n",
    "\n",
    "# 2. Load model checkpoint\n",
    "print(\"🔧 Loading model checkpoint...\")\n",
    "model_path = os.path.join(model_dir, \"word2vec_model.pth\")\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_config = checkpoint['model_config']\n",
    "training_config = checkpoint['training_config']\n",
    "training_info = checkpoint['training_info']\n",
    "\n",
    "print(f\"✅ Loaded model checkpoint\")\n",
    "print(f\"   - Embedding dim: {model_config['embedding_dim']}\")\n",
    "print(f\"   - Vocab size: {model_config['vocab_size']:,}\")\n",
    "print(f\"   - Previous loss: {training_info['final_loss']:.4f}\")\n",
    "print(f\"   - Previous epochs: {training_info['total_epochs']}\")\n",
    "\n",
    "# 3. Recreate model architecture and load state\n",
    "print(\"🏗️  Recreating model architecture...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleWord2Vec(model_config['vocab_size'], model_config['embedding_dim']).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"✅ Model loaded on {device}\")\n",
    "\n",
    "# 4. Set up optimizer with same parameters\n",
    "print(\"⚙️  Setting up optimizer...\")\n",
    "if training_config['optimizer'] == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                          lr=training_config['learning_rate'], \n",
    "                          weight_decay=training_config['weight_decay'])\n",
    "elif training_config['optimizer'] == 'adamw':\n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                           lr=training_config['learning_rate'], \n",
    "                           weight_decay=training_config['weight_decay'])\n",
    "else:  # sgd\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                         lr=training_config['learning_rate'],\n",
    "                         momentum=0.9, \n",
    "                         weight_decay=training_config['weight_decay'])\n",
    "\n",
    "print(f\"✅ Optimizer: {training_config['optimizer']} (lr={training_config['learning_rate']})\")\n",
    "\n",
    "# 5. Create training data with same parameters\n",
    "print(\"📊 Creating training data...\")\n",
    "contexts, targets = create_training_data(\n",
    "    indexed_words, \n",
    "    training_config['window_size'],\n",
    "    max_examples=1000000  # Same as before\n",
    ")\n",
    "\n",
    "dataset = TensorDataset(contexts, targets)\n",
    "dataloader = DataLoader(dataset, \n",
    "                       batch_size=training_config['batch_size'], \n",
    "                       shuffle=True)\n",
    "\n",
    "print(f\"✅ Training data ready: {len(contexts):,} examples\")\n",
    "print(f\"   - Window size: {training_config['window_size']}\")\n",
    "print(f\"   - Batch size: {training_config['batch_size']}\")\n",
    "\n",
    "print(\"🚀 Ready for continued training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up improved continued training parameters...\n",
      "📉 Reduced learning rate: 0.001\n",
      "✅ New optimizer ready with LR: 0.001\n",
      "🎯 Starting improved continued training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.5s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/wandb/run-20250611_105105-imngzszd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/imngzszd' target=\"_blank\">continued-training-improved</a></strong> to <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/simple-word2vec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/imngzszd' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/imngzszd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Starting from epoch 20 with loss 6.6165\n",
      "🔥 Training 1 more epoch with IMPROVED settings...\n",
      "  Batch  100: Current avg loss = 6.4767\n",
      "  Batch  200: Current avg loss = 6.4822\n",
      "  Batch  300: Current avg loss = 6.4773\n",
      "  Batch  400: Current avg loss = 6.4857\n",
      "  Batch  500: Current avg loss = 6.4837\n",
      "  Batch  600: Current avg loss = 6.4862\n",
      "  Batch  700: Current avg loss = 6.4890\n",
      "  Batch  800: Current avg loss = 6.4872\n",
      "  Batch  900: Current avg loss = 6.4894\n",
      "  Batch 1000: Current avg loss = 6.4886\n",
      "  Batch 1100: Current avg loss = 6.4891\n",
      "  Batch 1200: Current avg loss = 6.4870\n",
      "  Batch 1300: Current avg loss = 6.4866\n",
      "  Batch 1400: Current avg loss = 6.4863\n",
      "  Batch 1500: Current avg loss = 6.4868\n",
      "  Batch 1600: Current avg loss = 6.4857\n",
      "  Batch 1700: Current avg loss = 6.4854\n",
      "  Batch 1800: Current avg loss = 6.4839\n",
      "  Batch 1900: Current avg loss = 6.4811\n",
      "\n",
      "🎉 Improved continued training complete!\n",
      "📊 Results:\n",
      "   - Starting loss:  6.6165\n",
      "   - Final loss:     6.4802\n",
      "   - Improvement:    0.1364\n",
      "   - Early stopped:  False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>batch_loss</td><td>▄▃█▅▅▂▄▃▇▁▅▂▅▆▆▃▄▂▇</td></tr><tr><td>batches_completed</td><td>▁</td></tr><tr><td>final_continued_loss</td><td>▁</td></tr><tr><td>improved_learning_rate</td><td>▁</td></tr><tr><td>loss_improvement</td><td>▁</td></tr><tr><td>running_avg_loss</td><td>▁▄▁▆▅▆█▇███▇▆▆▇▆▆▅▃</td></tr><tr><td>starting_epoch</td><td>▁</td></tr><tr><td>starting_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>1900</td></tr><tr><td>batch_loss</td><td>6.64154</td></tr><tr><td>batches_completed</td><td>1954</td></tr><tr><td>continued_training</td><td>True</td></tr><tr><td>early_stopped</td><td>False</td></tr><tr><td>final_continued_loss</td><td>6.48016</td></tr><tr><td>improved_learning_rate</td><td>0.001</td></tr><tr><td>loss_improvement</td><td>0.13638</td></tr><tr><td>running_avg_loss</td><td>6.4811</td></tr><tr><td>starting_epoch</td><td>20</td></tr><tr><td>starting_loss</td><td>6.61654</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">continued-training-improved</strong> at: <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/imngzszd' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/simple-word2vec/runs/imngzszd</a><br> View project at: <a href='https://wandb.ai/joaopaesteves99-opensc/simple-word2vec' target=\"_blank\">https://wandb.ai/joaopaesteves99-opensc/simple-word2vec</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 🔧 SETUP FOR BETTER CONTINUED TRAINING\n",
    "print(\"🔧 Setting up improved continued training parameters...\")\n",
    "\n",
    "# 1. FIRST modify the learning rate in config\n",
    "training_config['learning_rate'] = 0.001  # Much smaller (20x reduction)\n",
    "print(f\"📉 Reduced learning rate: {training_config['learning_rate']}\")\n",
    "\n",
    "# 2. THEN recreate optimizer with new learning rate\n",
    "if training_config['optimizer'] == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                          lr=training_config['learning_rate'],  # Now uses 0.001\n",
    "                          weight_decay=training_config['weight_decay'])\n",
    "elif training_config['optimizer'] == 'adamw':\n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                           lr=training_config['learning_rate'], \n",
    "                           weight_decay=training_config['weight_decay'])\n",
    "else:  # sgd\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                         lr=training_config['learning_rate'],\n",
    "                         momentum=0.9, \n",
    "                         weight_decay=training_config['weight_decay'])\n",
    "\n",
    "# 3. Setup learning rate scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)\n",
    "\n",
    "print(f\"✅ New optimizer ready with LR: {training_config['learning_rate']}\")\n",
    "\n",
    "# 🎯 CONTINUE TRAINING FOR ONE MORE EPOCH\n",
    "print(\"🎯 Starting improved continued training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize wandb\n",
    "continued_run = wandb.init(\n",
    "    project=\"simple-word2vec\",\n",
    "    name=\"continued-training-improved\",\n",
    "    config=training_config,\n",
    "    tags=[\"continued\", \"improved-lr\"]\n",
    ")\n",
    "\n",
    "# Log starting point\n",
    "wandb.log({\n",
    "    'starting_epoch': training_info['total_epochs'],\n",
    "    'starting_loss': training_info['final_loss'],\n",
    "    'improved_learning_rate': training_config['learning_rate'],\n",
    "    'continued_training': True\n",
    "})\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "print(f\"📈 Starting from epoch {training_info['total_epochs']} with loss {training_info['final_loss']:.4f}\")\n",
    "print(f\"🔥 Training 1 more epoch with IMPROVED settings...\")\n",
    "\n",
    "# Training loop with early stopping\n",
    "start_epoch = training_info['total_epochs']\n",
    "starting_loss = training_info['final_loss']\n",
    "epoch_loss = 0\n",
    "batch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "for batch_contexts, batch_targets in dataloader:\n",
    "    if early_stop:\n",
    "        break\n",
    "        \n",
    "    batch_contexts = batch_contexts.to(device)\n",
    "    batch_targets = batch_targets.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch_contexts)\n",
    "    loss = loss_function(outputs, batch_targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch_loss += loss.item()\n",
    "    batch_count += 1\n",
    "    \n",
    "    # Check every 100 batches\n",
    "    if batch_count % 100 == 0:\n",
    "        current_avg_loss = epoch_loss / batch_count\n",
    "        print(f\"  Batch {batch_count:4d}: Current avg loss = {current_avg_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping check - INSIDE the loop!\n",
    "        if current_avg_loss > starting_loss * 1.02:  # Stop if 2% worse\n",
    "            print(f\"🛑 Early stopping! Loss increased to {current_avg_loss:.4f} (>{starting_loss * 1.02:.4f})\")\n",
    "            early_stop = True\n",
    "            \n",
    "        wandb.log({\n",
    "            'batch_loss': loss.item(),\n",
    "            'running_avg_loss': current_avg_loss,\n",
    "            'batch': batch_count\n",
    "        })\n",
    "\n",
    "# Calculate final results\n",
    "final_avg_loss = epoch_loss / batch_count  # Use batch_count in case of early stop\n",
    "improvement = starting_loss - final_avg_loss\n",
    "\n",
    "print(f\"\\n🎉 Improved continued training complete!\")\n",
    "print(f\"📊 Results:\")\n",
    "print(f\"   - Starting loss:  {starting_loss:.4f}\")\n",
    "print(f\"   - Final loss:     {final_avg_loss:.4f}\")\n",
    "print(f\"   - Improvement:    {improvement:.4f}\")\n",
    "print(f\"   - Early stopped:  {early_stop}\")\n",
    "\n",
    "# Log final results\n",
    "wandb.log({\n",
    "    'final_continued_loss': final_avg_loss,\n",
    "    'loss_improvement': improvement,\n",
    "    'early_stopped': early_stop,\n",
    "    'batches_completed': batch_count\n",
    "})\n",
    "\n",
    "continued_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Save updated model after continued training?\n",
      "==================================================\n",
      "🔄 Saving updated model...\n",
      "✅ Saved updated embeddings: workspace/word2vec_trained_model/model_2/word2vec_embeddings.npy\n",
      "✅ Saved vocabulary: workspace/word2vec_trained_model/model_2/word2vec_vocab.pkl\n",
      "✅ Saved updated model state: workspace/word2vec_trained_model/model_2/word2vec_model.pth\n",
      "✅ Saved updated model info: workspace/word2vec_trained_model/model_2/model_info.json\n",
      "\n",
      "🎉 Updated model saved successfully!\n",
      "📁 Location: workspace/word2vec_trained_model/model_2\n",
      "📈 Improvement: 0.1364 loss reduction\n",
      "\n",
      "📋 Summary:\n",
      "   Original model: 20 epochs, loss 6.6165\n",
      "   After continuation: 21 epochs, loss 6.4802\n",
      "   Improvement: 0.1364\n",
      "   Model status: Saved\n"
     ]
    }
   ],
   "source": [
    "# 💾 OPTIONALLY SAVE UPDATED MODEL\n",
    "print(\"💾 Save updated model after continued training?\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# You can run this cell to save the updated model\n",
    "save_updated_model = True  # Set to True to save, False to skip\n",
    "\n",
    "if save_updated_model:\n",
    "    print(\"🔄 Saving updated model...\")\n",
    "    \n",
    "    # Create new directory for updated model\n",
    "    updated_model_dir = \"workspace/word2vec_trained_model/model_2\"\n",
    "    os.makedirs(updated_model_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save updated embeddings\n",
    "    embeddings_path = os.path.join(updated_model_dir, \"word2vec_embeddings.npy\")\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    print(f\"✅ Saved updated embeddings: {embeddings_path}\")\n",
    "    \n",
    "    # 2. Save vocabulary (unchanged)\n",
    "    vocab_path = os.path.join(updated_model_dir, \"word2vec_vocab.pkl\")\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab_data, f)\n",
    "    print(f\"✅ Saved vocabulary: {vocab_path}\")\n",
    "    \n",
    "    # 3. Save updated model state\n",
    "    model_path = os.path.join(updated_model_dir, \"word2vec_model.pth\")\n",
    "    updated_model_save_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': model_config,\n",
    "        'training_config': training_config,\n",
    "        'training_info': updated_training_info,\n",
    "        'original_training_info': training_info  # Keep original for reference\n",
    "    }\n",
    "    \n",
    "    torch.save(updated_model_save_data, model_path)\n",
    "    print(f\"✅ Saved updated model state: {model_path}\")\n",
    "    \n",
    "    # 4. Save updated model info\n",
    "    config_path = os.path.join(updated_model_dir, \"model_info.json\")\n",
    "    updated_model_info = {\n",
    "        \"model_name\": \"SimpleWord2Vec_CBOW_Continued\",\n",
    "        \"embedding_dimensions\": model_config['embedding_dim'],\n",
    "        \"vocabulary_size\": model_config['vocab_size'],\n",
    "        \"training_examples\": len(contexts),\n",
    "        \"final_loss\": float(final_avg_loss),\n",
    "        \"total_epochs\": new_total_epochs,\n",
    "        \"continued_training\": True,\n",
    "        \"original_epochs\": training_info['total_epochs'],\n",
    "        \"original_loss\": training_info['final_loss'],\n",
    "        \"loss_improvement\": float(training_info['final_loss'] - final_avg_loss),\n",
    "        \"hyperparameters\": training_config,\n",
    "        \"dataset\": \"text8\",\n",
    "        \"continued_date\": updated_training_info['continued_training_date'],\n",
    "        \"notes\": \"Model continued training for 1 additional epoch\"\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(updated_model_info, f, indent=2)\n",
    "    print(f\"✅ Saved updated model info: {config_path}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Updated model saved successfully!\")\n",
    "    print(f\"📁 Location: {updated_model_dir}\")\n",
    "    print(f\"📈 Improvement: {training_info['final_loss'] - final_avg_loss:.4f} loss reduction\")\n",
    "    \n",
    "else:\n",
    "    print(\"⏭️  Skipping model save (set save_updated_model=True to save)\")\n",
    "\n",
    "print(f\"\\n📋 Summary:\")\n",
    "print(f\"   Original model: {training_info['total_epochs']} epochs, loss {training_info['final_loss']:.4f}\")\n",
    "print(f\"   After continuation: {new_total_epochs} epochs, loss {final_avg_loss:.4f}\")\n",
    "print(f\"   Improvement: {training_info['final_loss'] - final_avg_loss:.4f}\")\n",
    "print(f\"   Model status: {'Saved' if save_updated_model else 'Not saved'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
